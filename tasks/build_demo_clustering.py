import random
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
import numpy as np
import json
import matplotlib.pyplot as plt
import argparse
from src.utils import *
import os
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from typing import List, Dict, Tuple


def build_demo_self_prompt_cot(demo):
    self_cot = "Let's think step by step:\n"
    for j, hop in enumerate(demo["hops"]):
        # evidence only
        evidence = hop['evidence'].replace('as stated in the passage', '').replace('The passage states that ', '')
        evidence = evidence.replace('as mentioned in the passage', '').replace('The passage mentions that ', '').replace(' in the passage', '')
        evidence = evidence.replace("(I have not included the entire entry in this post)", "")
        evidence = clean_passage(evidence)
        evidence = evidence.replace("\n", "")

        self_cot += f"Step {j + 1}: {evidence}\n"

    self_cot += f"Therefore, the final answer in just one entity is: {demo['answer']}\n"

    return {
        "question": demo["question"].replace("<bot>:", "").replace("<human>:", "").strip(),
        "answer": demo["answer"],
        "cot": self_cot,
        "hop_type": demo["hop_type"]
    }


def build_demo_self_prompt(demo):
    return {
        "question": demo["question"],
        "answer": f"{demo['answer']}, because {demo['evidence']}",
    }


def build_demo_auto_cot(demo):
    cot = f"Let's think step by step. {demo['cot']}" if "cot" in demo else demo["prompt"][-2][8:]

    return {
        "question": demo["question"],
        "answer": demo['answer'],
        "cot": cot,
        "raw_pred": demo["response"]
    }


def cluster_and_embed(num_clusters: int, corpus_embeddings, seed: int = 42):
    # cluster the corpus
    clustering_model = KMeans(n_clusters=num_clusters, random_state=seed)
    clustering_model.fit(corpus_embeddings)
    cluster_labels = clustering_model.labels_

    # put corpus embeddings according to its cluster_labels into different clusters
    clusters = [[] for _ in range(num_clusters)]
    clusters_idx: List[List[int]] = [[] for _ in range(num_clusters)]
    for i, label in enumerate(cluster_labels):
        clusters[label].append(corpus_embeddings[i])
        clusters_idx[label].append(i)

    # concatenate the embeddings in each cluster
    cluster_embeddings = []
    for i in range(num_clusters):
        cluster_embeddings.append(np.stack(clusters[i], axis=0))

    return cluster_embeddings, clusters_idx, clustering_model


def parse_arguments():
    parser = argparse.ArgumentParser(description="self-CoT")
    parser.add_argument(
        "--task", type=str, default="hover",
        choices=list(DEFAULT_DEV_PATHS.keys()), help="dataset used for experiment"
    )
    parser.add_argument("--input_gen_file", type=str)
    parser.add_argument("--dataset_file", type=str, default=None)
    parser.add_argument(
        "--dataset_path", type=str, default=None, help="dataset used for experiment"
    )

    parser.add_argument(
        "--model_name", type=str, default="gpt-3.5-turbo-0301", help="dataset used for experiment"
    )

    parser.add_argument(
        "--method", type=str, default="zero-shot-cot",
        choices=["self-prompt", "auto-cot", "self-prompt-cot"],
        help="method used for experiment"
    )

    parser.add_argument(
        "--sampling", type=str, default="retrieve_in_cluster",
        choices=["retrieve_in_cluster", "cluster_center", "random", "retrieve", "retrieve_in_type_cluster"],
        help="sampling method"
    )

    parser.add_argument(
        "--num_clusters", type=int, default=8, help="number of clusters"
    )

    parser.add_argument(
        "--output_file", type=str, default=None,
        help="use the reasoning chains generated by zero-shot-cot."
    )
    parser.add_argument(
        "--demo_save_dir", type=str, default="demos", help="where to save the constructed demonstrations"
    )
    parser.add_argument("--random_seed", type=int, default=42, help="random seed")
    parser.add_argument(
        "--encoder", type=str, default="all-MiniLM-L6-v2", help="which sentence-transformer encoder for clustering"
    )
    parser.add_argument(
        "--do_test", action="store_true", help="whether to test the model"
    )
    parser.add_argument(
        "--device", type=str, default="cuda:0", help="device to use"
    )
    parser.add_argument(
        "--clustering_content", type=str, default="Q",
        choices=["Q", "QA"], help="Q: question, QA: question + answer")
    parser.add_argument(
        "--encoding_batch_size", type=int, default=128, help="batch size for encoding")
    parser.add_argument(
        "--flag", type=str, default=None, help="flag for different experiments")
    parser.add_argument(
        "--limit_dataset_size", type=int, default=0, help="limit the dataset size for debugging")
    parser.add_argument(
        "--limit_gen_size", type=int, default=0, help="limit the dataset size for debugging")


    args = parser.parse_args()

    if args.dataset_path is None:
        args.dataset_path = DEFAULT_PATHS[args.task]
        print(f"[{print_now(1)}] Using default dataset path {args.dataset_path}...")

    args.demo_save_dir = os.path.join(args.demo_save_dir, args.task)
    args.demo_save_dir = os.path.join(args.demo_save_dir, args.model_name)

    if args.dataset_file is None:
        print(f"[{print_now(1)}] Loading default file for {args.task}...")
        if args.do_test:
            args.dataset_file = os.path.join(args.dataset_path, "processed_test.json")
        else:
            args.dataset_file = os.path.join(args.dataset_path, "processed_dev.json")
        # args.dataset_file = os.path.join(args.dataset_path, "processed_test.json")
        if not os.path.exists(args.dataset_file):
            raise ValueError(f"Default file {args.dataset_file} does not exist.")
    else:
        print(f"[{print_now(1)}] Loading file {args.dataset_file} for {args.task}...")

    if args.output_file is None:
        if args.do_test:
            args.output_file = f"{args.method}_{args.sampling}_demos-{args.num_clusters}_test.json"
        else:
            args.output_file = f"{args.method}_{args.sampling}_demos-{args.num_clusters}.json"

    if args.flag is not None:
        args.output_file = args.flag + "_" + args.output_file

    print(f"output_file: {args.output_file}")

    os.makedirs(args.demo_save_dir, exist_ok=True)

    return args


def main(args):
    fix_seed(args.random_seed)
    encoder = SentenceTransformer(args.encoder)

    corpus = []
    raw_gen_dataset = load_json_and_jsonl_file(args.input_gen_file)
    raw_dataset = load_json_and_jsonl_file(args.dataset_file)
    queries: List[str] = []
    answers: List[List[str]] = []
    for data in raw_dataset:
        queries.append(data["question"])
        raw_ans = data["answer"]
        if isinstance(raw_ans, str):
            answers.append([raw_ans])
        else:
            answers.append(raw_ans)

    if args.limit_dataset_size > 0:
        queries = queries[:args.limit_dataset_size]
        answers = answers[:args.limit_dataset_size]

    gen_dataset: List[Dict] = []
    gen_labels: List[int] = []

    if args.method == "self-prompt-cot":
        # self-prompt-cot dataset
        for key in raw_gen_dataset:
            for data in raw_gen_dataset[key]:
                gen_dataset.append(data)
                gen_labels.append(TYPE_MAPPING[key])
    else:
        # self-prompt or auto-cot dataset
        for data in raw_gen_dataset:
            gen_dataset.append(data)

    if args.limit_gen_size > 0:
        gen_dataset = gen_dataset[:args.limit_gen_size]


    for data in gen_dataset:
        if args.clustering_content == "QA":
            corpus.append(data["question"] + " " + data["answer"])
        else:
            corpus.append(data["question"])

    # encode the corpus
    print(f"Encoding the corpus of {len(corpus)} examples...")
    corpus_embeddings = encoder.encode(corpus, show_progress_bar=True, device=args.device, batch_size=args.encoding_batch_size)
    print(f"Encoding the queries of {len(queries)} examples...")
    query_embeddings = encoder.encode(queries, show_progress_bar=True, device=args.device, batch_size=args.encoding_batch_size)

    num_clusters = args.num_clusters
    gen_labels: np.ndarray = np.array(gen_labels)

    cluster_embeddings, clusters_idx, clustering_model = cluster_and_embed(num_clusters, corpus_embeddings, args.random_seed)

    output_demos = []

    # for each query embedding, find the index with max cosine similarity in each cluster
    if args.sampling == "retrieve_in_cluster":
        for i in tqdm(range(len(queries)), desc="Constructing demos..."):
            query_embedding = query_embeddings[i]
            query_embedding = query_embedding.reshape(1, -1)
            most_similar_idx = []
            similar_scores = []
            for cluster_id in range(num_clusters):
                cluster_embedding = cluster_embeddings[cluster_id]
                cos_sim = cosine_similarity(query_embedding, cluster_embedding)[0]
                max_score = np.max(cos_sim)
                # avoid the case where the query is exactly the same as one of the demos
                if (1.0 - max_score) < 1e-5:
                    idx_ranking = np.argsort(cos_sim)[::-1]
                    second_highest_idx = idx_ranking[1]
                    most_similar_idx.append(clusters_idx[cluster_id][second_highest_idx])
                    similar_scores.append(cos_sim[second_highest_idx])
                else:
                    most_similar_idx.append(clusters_idx[cluster_id][np.argmax(cos_sim)])
                    similar_scores.append(max_score)
            # sort most_similar_idx according to the similarity score
            most_similar_idx = sorted(enumerate(most_similar_idx), key=lambda x: similar_scores[x[0]], reverse=True)
            similar_scores = sorted(similar_scores, reverse=True)
            raw_demos = [gen_dataset[idx_tup[1]] for idx_tup in most_similar_idx]
            demos = []
            for idx, raw_demo in enumerate(raw_demos):
                if args.method == "self-prompt-cot":
                    demo = build_demo_self_prompt_cot(raw_demo)
                elif args.method == "self-prompt":
                    demo = build_demo_self_prompt(raw_demo)
                elif args.method == "auto-cot":
                    demo = build_demo_auto_cot(raw_demo)
                else:
                    raise ValueError(f"Invalid method: {args.method}")
                demo["question"] = demo["question"].replace("\n", "")
                demo["score"] = str(similar_scores[idx])
                demos.append(demo)

            example = {
                "question": queries[i].replace("\n", ""),
                "answer": answers[i],
                "demos": demos
            }
            output_demos.append(example)

    # find the closest qa pair to the cluster center
    elif args.sampling == "cluster_center":
        cluster_centers = clustering_model.cluster_centers_
        most_similar_idx = []
        similar_scores = []
        for cluster_id in range(num_clusters):
            cluster_center_embedding = cluster_centers[cluster_id]
            cluster_embedding = cluster_embeddings[cluster_id]
            cos_sim = cosine_similarity(cluster_center_embedding.reshape(1, -1), cluster_embedding)[0]
            max_score = np.max(cos_sim)
            most_similar_idx.append(clusters_idx[cluster_id][np.argmax(cos_sim)])
            similar_scores.append(max_score)
        # sort most_similar_idx according to the similarity score
        most_similar_idx = sorted(enumerate(most_similar_idx), key=lambda x: similar_scores[x[0]], reverse=True)
        similar_scores = sorted(similar_scores, reverse=True)
        raw_demos = [gen_dataset[idx_tup[1]] for idx_tup in most_similar_idx]
        demos = []
        for idx, raw_demo in enumerate(raw_demos):
            if args.method == "self-prompt-cot":
                demo = build_demo_self_prompt_cot(raw_demo)
            elif args.method == "self-prompt":
                demo = build_demo_self_prompt(raw_demo)
            elif args.method == "auto-cot":
                demo = build_demo_auto_cot(raw_demo)
            else:
                raise ValueError(f"Invalid method: {args.method}")
            demo["score"] = str(similar_scores[idx])
            demos.append(demo)

        for i in tqdm(range(len(queries))):
            example = {
                "question": queries[i],
                "answer": answers[i],
                "demos": demos
            }
            output_demos.append(example)
    elif args.sampling == "retrieve":
        for i in tqdm(range(len(queries)), desc="Constructing demos..."):
            query_embedding = query_embeddings[i]
            query_embedding = query_embedding.reshape(1, -1)
            cos_sim = cosine_similarity(query_embedding, corpus_embeddings)[0]
            idx_ranking = np.argsort(cos_sim)[::-1]
            most_similar_idx = idx_ranking[:num_clusters]
            raw_demos = [gen_dataset[idx] for idx in most_similar_idx]
            similar_scores = [cos_sim[idx] for idx in most_similar_idx]
            demos = []
            for idx, raw_demo in enumerate(raw_demos):
                if args.method == "self-prompt-cot":
                    demo = build_demo_self_prompt_cot(raw_demo)
                elif args.method == "self-prompt":
                    demo = build_demo_self_prompt(raw_demo)
                elif args.method == "auto-cot":
                    demo = build_demo_auto_cot(raw_demo)
                else:
                    raise ValueError(f"Invalid method: {args.method}")
                demo["score"] = str(similar_scores[idx])
                demos.append(demo)
            example = {
                "question": queries[i],
                "answer": answers[i],
                "demos": demos
            }
            output_demos.append(example)
    elif args.sampling == "random":
        print("Random sampling with seed 42 ...")
        random.seed(2022)
        for i in tqdm(range(len(queries)), desc="Constructing demos..."):
            raw_demos = random.sample(gen_dataset, num_clusters)
            demos = []
            for idx, raw_demo in enumerate(raw_demos):
                if args.method == "self-prompt-cot":
                    demo = build_demo_self_prompt_cot(raw_demo)
                elif args.method == "self-prompt":
                    demo = build_demo_self_prompt(raw_demo)
                elif args.method == "auto-cot":
                    demo = build_demo_auto_cot(raw_demo)
                else:
                    raise ValueError(f"Invalid method: {args.method}")
                demos.append(demo)

            example = {
                "question": queries[i],
                "answer": answers[i],
                "demos": demos
            }
            output_demos.append(example)
    elif args.sampling == "retrieve_in_type_cluster":
        cluster_embeddings = []
        cluster_labels = []
        gen_labels: np.ndarray = np.array(gen_labels)
        cluster_dataset = []
        knn_model = KNeighborsClassifier(n_neighbors=5)
        knn_model.fit(corpus_embeddings, gen_labels)
        for type_id in TYPE_MAPPING.values():
            # get the embeddings of the demos of the same type
            type_mask = gen_labels == type_id
            type_embeddings = corpus_embeddings[type_mask]
            cluster_dataset.append([gen_dataset[i] for i, mask in enumerate(type_mask) if mask])
            type_cluster_embeddings, type_cluster_idx, _ = cluster_and_embed(num_clusters, type_embeddings)
            cluster_embeddings.append(type_cluster_embeddings)
            cluster_labels.append(type_cluster_idx)

        # iterate over questions, determine the type of the question with knn and find the most similar demo in each cluster
        output_demos = []
        for i in tqdm(range(len(queries)), desc="Constructing demos..."):
            query_embedding = query_embeddings[i]
            query_embedding = query_embedding.reshape(1, -1)
            # find the type of the question
            type_id = knn_model.predict(query_embedding)[0]
            # find the most similar demo in each cluster
            most_similar_idx = []
            similar_scores = []
            for cluster_id in range(num_clusters):
                cluster_embedding = cluster_embeddings[type_id][cluster_id]
                cos_sim = cosine_similarity(query_embedding, cluster_embedding)[0]
                max_score = np.max(cos_sim)
                if (1.0 - max_score) < 1e-5:
                    idx_ranking = np.argsort(cos_sim)[::-1]
                    second_highest_idx = idx_ranking[1]
                    most_similar_idx.append(cluster_labels[type_id][cluster_id][second_highest_idx])
                    similar_scores.append(cos_sim[second_highest_idx])
                else:
                    most_similar_idx.append(cluster_labels[type_id][cluster_id][np.argmax(cos_sim)])
                    similar_scores.append(max_score)
            # find the most similar demo among all clusters
            most_similar_idx = sorted(enumerate(most_similar_idx), key=lambda x: similar_scores[x[0]], reverse=True)
            similar_scores = sorted(similar_scores, reverse=True)
            raw_demos = [cluster_dataset[type_id][idx_tup[1]] for idx_tup in most_similar_idx]
            demos = []
            for idx, raw_demo in enumerate(raw_demos):
                demo = build_demo_self_prompt_cot(raw_demo)
                demo["score"] = str(similar_scores[idx])
                demos.append(demo)
            output_demos.append({
                "question": queries[i],
                "answer": answers[i],
                "demos": demos
            })
    else:
        raise ValueError(f"Invalid sampling method: {args.sampling}")

    with open(os.path.join(args.demo_save_dir, args.output_file), "w") as f:
        json.dump(output_demos, f, indent=4, ensure_ascii=False)


if __name__ == "__main__":
    args = parse_arguments()
    main(args)
